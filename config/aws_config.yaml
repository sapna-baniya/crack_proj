# AWS Configuration for Crack Detection Pipeline
# This file contains all AWS-specific settings

# AWS Account Settings
account:
  account_id: "905418031724"  # Get from AWS Console (top right, click your name)
  region: "us-east-1"  # Change if needed: us-west-2, eu-west-1, etc.

# AWS Credentials
# Note: Don't put actual credentials here! Use 'aws configure' instead
# This is just for reference
credentials:
  # Run: aws configure
  # Then enter your Access Key ID and Secret Access Key
  profile: "default"  # AWS CLI profile to use

# S3 Bucket Configuration
s3:
  # Bucket names (will be created by Terraform)
  raw_data_bucket: "crack-detection-raw-data"
  processed_data_bucket: "crack-detection-processed-data"
  model_artifacts_bucket: "crack-detection-model-artifacts"
  logs_bucket: "crack-detection-logs"
  
  # S3 Settings
  versioning_enabled: true
  encryption_enabled: true
  lifecycle_rules:
    logs_retention_days: 90
    old_data_to_glacier_days: 180

# SageMaker Configuration
sagemaker:
  # IAM Role (will be created by Terraform or manually)
  role_arn: "arn:aws:iam::YOUR_ACCOUNT_ID:role/SageMakerExecutionRole"
  
  # Training Configuration
  training:
    instance_type: "ml.p3.2xlarge"  # GPU instance for training
    instance_count: 1
    volume_size_gb: 50
    max_runtime_seconds: 86400  # 24 hours
    use_spot_instances: false  # Set to true for 70% cost savings
    max_wait_time_seconds: 86400  # For spot instances
  
  # Inference Configuration
  inference:
    instance_type: "ml.t2.medium"  # CPU instance for inference
    initial_instance_count: 1
    max_concurrent_transforms: 10
  
  # Processing Configuration
  processing:
    instance_type: "ml.m5.xlarge"
    instance_count: 1
    volume_size_gb: 30

# ECR (Elastic Container Registry)
ecr:
  repository_name: "crack-detection-models"
  image_tag_mutability: "MUTABLE"
  scan_on_push: true

# DynamoDB Configuration
dynamodb:
  metadata_table: "crack-detection-metadata"
  billing_mode: "PAY_PER_REQUEST"  # or "PROVISIONED"
  
  # If using PROVISIONED billing
  provisioned_throughput:
    read_capacity_units: 5
    write_capacity_units: 5

# API Gateway Configuration
api_gateway:
  api_name: "crack-detection-api"
  stage: "prod"
  throttle_settings:
    rate_limit: 1000  # requests per second
    burst_limit: 2000

# Lambda Configuration
lambda:
  # Inference Lambda
  inference_function:
    function_name: "crack-detection-inference"
    runtime: "python3.9"
    memory_size: 512  # MB
    timeout: 30  # seconds
    
  # Preprocessing Lambda
  preprocessing_function:
    function_name: "crack-detection-preprocess"
    runtime: "python3.9"
    memory_size: 1024  # MB
    timeout: 60  # seconds

# CloudWatch Configuration
cloudwatch:
  log_group: "/aws/sagemaker/crack-detection"
  log_retention_days: 30
  metrics_namespace: "CrackDetection"
  
  # Alarms
  alarms:
    model_error_rate:
      threshold: 5  # percent
      evaluation_periods: 2
      period: 300  # seconds
    
    endpoint_latency:
      threshold: 1000  # milliseconds
      evaluation_periods: 2
      period: 60  # seconds

# AWS Glue Configuration (for data catalog)
glue:
  database_name: "crack_detection_db"
  crawler_name: "crack-detection-crawler"
  crawler_schedule: "cron(0 2 * * ? *)"  # Daily at 2 AM UTC

# VPC Configuration (optional, for secure deployment)
vpc:
  enabled: false  # Set to true if you want to use VPC
  vpc_id: "vpc-xxxxxxxxx"
  subnet_ids:
    - "subnet-xxxxxxxxx"
    - "subnet-yyyyyyyyy"
  security_group_ids:
    - "sg-xxxxxxxxx"

# Cost Optimization Settings
cost_optimization:
  use_spot_instances: false  # Recommended for training
  stop_idle_resources: true
  s3_intelligent_tiering: true
  cloudwatch_log_retention: 30  # days
  
  # Budget Alerts
  budget:
    monthly_limit: 50  # USD
    alert_threshold: 80  # percent of limit

# Tags (applied to all resources)
tags:
  Project: "CrackDetection"
  Environment: "dev"  # or "prod"
  Owner: "YOUR_NAME"
  ManagedBy: "Terraform"
  CostCenter: "Research"

# Backup Configuration
backup:
  enabled: true
  s3_versioning: true
  dynamodb_point_in_time_recovery: true
  backup_retention_days: 30

# Security Settings
security:
  # S3 Bucket Policies
  s3_encryption: "AES256"  # or "aws:kms"
  block_public_access: true
  
  # IAM
  require_mfa: false  # Set to true for production
  
  # Secrets Manager (for sensitive data)
  use_secrets_manager: false
  secrets_manager_secret_name: "crack-detection/secrets"

# Monitoring & Alerting
monitoring:
  enabled: true
  
  # SNS Topic for alerts
  sns_topic_name: "crack-detection-alerts"
  alert_email: "YOUR_EMAIL@example.com"
  
  # Metrics to track
  custom_metrics:
    - "ModelAccuracy"
    - "InferenceLatency"
    - "DataQualityScore"
    - "PredictionCount"

# Multi-Region Configuration (optional)
multi_region:
  enabled: false
  regions:
    primary: "us-east-1"
    secondary: "us-west-2"

# Development vs Production Settings
environments:
  dev:
    instance_types:
      training: "ml.m5.xlarge"  # Cheaper for dev
      inference: "ml.t2.small"
    auto_shutdown: true
    
  prod:
    instance_types:
      training: "ml.p3.2xlarge"  # GPU for production
      inference: "ml.c5.xlarge"
    auto_shutdown: false
    high_availability: true

# Notes and Best Practices
# 
# 1. Never commit AWS credentials to git
# 2. Use 'aws configure' to set up credentials
# 3. Use IAM roles when possible instead of access keys
# 4. Enable MFA for production accounts
# 5. Regularly rotate access keys
# 6. Set up billing alerts
# 7. Use spot instances for training to save costs
# 8. Stop resources when not in use
# 9. Enable S3 versioning for important data
# 10. Review CloudWatch logs regularly

# How to Use This File:
# 
# 1. Update YOUR_AWS_ACCOUNT_ID with your actual account ID
# 2. Update YOUR_EMAIL@example.com with your email
# 3. Adjust instance types based on your needs and budget
# 4. Run 'aws configure' to set up credentials
# 5. Deploy infrastructure: cd infrastructure/terraform && terraform apply